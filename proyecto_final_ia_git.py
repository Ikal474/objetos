# -*- coding: utf-8 -*-
"""proyecto_final_IA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/135HZKJLuh3FKYuIIMlTFjx9lyi5mR6a3
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# ruta
DIR_DATASET = '/content/drive/MyDrive/dataset_cosas'
IMG_SIZE = (224, 224)
BATCH_SIZE = 32

print("carpeta:")
print(os.listdir(DIR_DATASET))

train_ds = tf.keras.utils.image_dataset_from_directory(
    DIR_DATASET,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    DIR_DATASET,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

class_names = train_ds.class_names
print("Clases detectadas:", class_names)

# Mejor rendimiento (prefetch)
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.15),
    layers.RandomZoom(0.15),
    layers.RandomContrast(0.2),
], name="data_augmentation")

#aqui cargamos el eficentnetb0

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input

base_model = EfficientNetB0(
    include_top=False,
    weights="imagenet",
    input_shape=IMG_SIZE + (3,)
)
# aqui se congela
base_model.trainable = False

inputs = tf.keras.Input(shape=IMG_SIZE + (3,))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.25)(x)
x = layers.Dense(128, activation="relu")(x)
outputs = layers.Dense(len(class_names), activation="softmax")(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

hist = model.fit(train_ds,validation_data=val_ds,epochs=5)

import matplotlib.pyplot as plt
def preparar_imagen(ruta):
    im = Image.open(ruta).convert("RGB")
    im = im.resize(IMG_SIZE)
    arr = np.array(im)

    plt.imshow(arr)
    plt.axis("off")
    plt.show()

    x = np.expand_dims(arr, axis=0)
    x = preprocess_input(x)

    return x

#aqui hicimos una prueba local del entrenamiento para medir su funcionamiento antes de subir a streamlit
x = preparar_imagen("calc.JPG")
pred = model.predict(x)[0]
idx = np.argmax(pred)

print("Probabilidades:", pred)
print("Predicci√≥n:", class_names[idx])

from google.colab import files
model.save("mi_modelo.keras")#Archivo que guardaremos para usarlo en streamlit